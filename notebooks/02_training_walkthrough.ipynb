#!/usr/bin/env python
# coding: utf-8

# # DeepRAG Training Walkthrough

# This notebook demonstrates the training process for DeepRAG, starting with DSI pre-training.

# ## 1. Setup

# In[1]:


# Ensure the project is installed
# !pip install -e ".[dev,viz]"


# In[2]:


import torch
from hydra import compose, initialize

from deprag.configs.config import register_configs
from deprag.trainers.train_dsi import train_dsi


# ## 2. DSI Pre-training

# The first step is to pre-train the Differentiable Search Index (DSI). The goal is to teach the model to map natural language queries to specific document identifiers (doc IDs).

# ### Configuration
# We will use the `synthetic` dataset and the `dsi_small` model for a quick demonstration. We'll run the training for just a few steps.

# In[3]:


register_configs()
with initialize(config_path="../deprag/configs", version_base="1.3"):
    cfg = compose(
        config_name="defaults",
        overrides=[
            "data=synthetic",
            "model=dsi_small",
            "train=dsi_pretrain",
            "training.max_steps=10", # Run for only 10 steps
            "training.batch_size=1",
            "training.logging_steps=1",
            "device=cpu",
        ],
    )


# ### Prepare the Data
# Before training, we need to ensure the document store for our synthetic data is created.

# In[4]:


# !python cli.py prepare-data --config-name=data/synthetic


# ### Run the Training
# Now, we can call the `train_dsi` function with our configuration. This will:
# 1. Load the DSI model and tokenizer.
# 2. Load the synthetic dataset.
# 3. Create a `DSICollator` to prepare batches.
# 4. Set up an optimizer and learning rate scheduler.
# 5. Run the training loop for 10 steps.

# In[5]:


print("Starting DSI pre-training for 10 steps...")
train_dsi(cfg)
print("\nDSI pre-training finished.")


# ## 3. Agent RL Training (Conceptual)

# After pre-training the DSI, the next step is to train the agent using reinforcement learning (PPO).

# The `train_agent` function orchestrates this process. In each step, it:
# 1. **Rolls out trajectories**: The agent generates responses to queries, deciding whether to use the `[RET]` token.
# 2. **Computes rewards**: The generated responses are compared to ground truth answers to calculate a task reward. A penalty is applied for each retrieval.
# 3. **Performs a PPO update**: The `PPOTrainer` uses the collected trajectories to update the agent's policy and value function.

# Running this live in a notebook is complex, but you can run it from the command line:

# ```bash
# python cli.py train-agent --config-name=train/agent_ppo data=synthetic model=agent_small training.max_steps=5
# ```

# ## 4. Joint Training (Conceptual)

# The final, most advanced stage is joint training. Here, both the DSI and the agent are updated simultaneously.

# - The agent is updated via PPO as before.
# - The DSI is updated based on the queries the agent chose to make during the rollout. The gradients from the agent's task loss can flow back to the DSI, fine-tuning it to retrieve documents that are most useful for the agent.

# This is the most powerful feature of DeepRAG but also the most complex to implement and tune.

# You can run the conceptual version from the CLI:
# ```bash
# python cli.py train-joint
# ```
